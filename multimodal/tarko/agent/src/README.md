# @tarko/agent

An event-stream driven meta agent framework for building effective multimodal Agents.

## Overview

`@tarko/agent` is the core framework that powers intelligent agents capable of reasoning, tool usage, and multimodal interactions. Built for developers who need reliable, production-ready AI agents with full control over execution flow.

ðŸŽ¯ **ç²¾ç»† Context Engineering**

åœ¨ Tarko ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ Agent Loop çš„å¤šæ¨¡æ€å†…å®¹ã€Run Loop çš„ä¸Šä¸‹æ–‡åŽ‹ç¼©ã€MCP Result ç­‰éƒ½è¿›è¡Œäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œè¿™æžå¤§åœ°å‡å°‘äº†ä¸Šå±‚å¼€å‘è€…çš„å¼€å‘è´Ÿæ‹…ã€‚

ðŸ”— **å¤šæ¨¡åž‹å…¼å®¹çš„ Tool Call**

Tarko ä»¥ Tool Call ä¸ºåŸºç¡€ï¼Œå¹¶æä¾›äº†å¼€ç®±å³ç”¨çš„ Model Providerã€å¤š Model Provider ç­‰æœºåˆ¶ï¼Œè®©ä½ èƒ½å¤Ÿè½»æ¾åœ°åˆ‡æ¢æ¨¡åž‹ï¼ŒåŒæ—¶ï¼Œå¸¦æ¥äº†å¤šç§ Tool Call Engine çš„æ”¯æŒï¼Œå³ä½¿æ¨¡åž‹ä¸æ”¯æŒ Tool Callï¼Œä½ ä¹Ÿèƒ½å¤Ÿå®žçŽ°è‡ªå®šä¹‰ Tool Call è§£æžæ¥å®Œæˆæ”¯æŒã€‚

ðŸ“Š **ç¨³å®šæ€§ä¸Žè§‚æµ‹**

åœ¨ Tarko ä¸­ï¼Œèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶å°† Agent æ‰€ä¾èµ–çš„çŽ¯å¢ƒä¿å­˜ä¸º Snapshotï¼ŒæŽ¥ç€ï¼Œå¯ä»¥åŸºäºŽ Snapshot æ¥å›žæ”¾ Agentï¼Œä¸ä»…ç”¨äºŽè°ƒè¯•ï¼Œä¹Ÿå¯ä»¥ä¿éšœ Contextã€ä¸Žæœ€ç»ˆçš„ Response çš„ç¡®å®šæ€§ã€‚

ðŸš€ **å¼ºå¤§çš„æ‹“å±•èƒ½åŠ›**

åœ¨ Tarko ä¸­ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ Agent Hooksï¼Œè®©ä½ èƒ½å¤Ÿå¿«é€Ÿåœ°æ‹“å±•èƒ½åŠ›ï¼Œå¿«é€Ÿå®žçŽ°åž‚ç›´åœºæ™¯çš„ Agentï¼Œå¦‚ DeepResearch Agentã€GUI Agentã€Coding Agent ç­‰ã€‚

ðŸ’¨ **Protocol é©±åŠ¨**

Tarko ä¸­çš„ Contextã€Memory ä¸Ž Web UI å®Œå…¨åŸºäºŽä¸€å¥—æ ‡å‡†çš„åè®®é©±åŠ¨ï¼Œå› æ­¤ï¼Œé€šè¿‡ Tarko å¼€å‘ Agent å°†èƒ½å¤Ÿäº«å—å¼€ç®±å³ç”¨çš„ Web UIï¼Œä¹Ÿæ”¯æŒåŸºäºŽåè®®è‡ªå®šä¹‰å®žçŽ°ã€‚

ðŸŒŸ **å¼€æºé¡¹ç›®é‡‡çº³**

Tarko å·²ç»é©±åŠ¨äº† Agent TARSã€UI-TARS Desktop ç­‰å¼€æºé¡¹ç›®çš„å»ºè®¾ï¼Œè¿™äº›é¡¹ç›®åœ¨ Github ä¸ŠèŽ·å–äº†è¶…è¿‡ 15k çš„ Starsã€‚

## Quick Start

### Installation

```bash
npm install @tarko/agent
# or
pnpm add @tarko/agent
```

### Basic Usage

```typescript
import { Agent } from '@tarko/agent';

// Create an agent with custom instructions
const agent = new Agent({
  instructions: 'You are a helpful coding assistant.',
  model: {
    provider: 'openai',
    id: 'gpt-4o'
  },
  maxIterations: 5
});

// Simple text interaction
const response = await agent.run('Help me debug this JavaScript error');
console.log(response.content);

// Streaming response
for await (const event of await agent.run({
  input: 'Explain async/await in JavaScript',
  stream: true
})) {
  if (event.type === 'assistant_message_chunk') {
    process.stdout.write(event.content);
  }
}
```

### With Tools

```typescript
import { Agent, Tool } from '@tarko/agent';

// Define a custom tool
const weatherTool: Tool = {
  name: 'get_weather',
  description: 'Get current weather for a location',
  parameters: {
    type: 'object',
    properties: {
      location: { type: 'string', description: 'City name' }
    },
    required: ['location']
  },
  execute: async ({ location }) => {
    // Your weather API logic here
    return `Weather in ${location}: 22Â°C, sunny`;
  }
};

const agent = new Agent({
  instructions: 'You can check weather using the available tools.',
  tools: [weatherTool]
});

const result = await agent.run('What\'s the weather like in Tokyo?');
```

### Multimodal Input

```typescript
const response = await agent.run({
  input: [
    { type: 'text', text: 'What do you see in this image?' },
    { 
      type: 'image_url', 
      image_url: { url: 'data:image/jpeg;base64,...' } 
    }
  ]
});
```

## API Reference

### Agent Constructor

```typescript
interface AgentOptions {
  instructions?: string;           // System prompt
  tools?: Tool[];                 // Available tools
  model?: ModelConfig;            // LLM configuration
  maxIterations?: number;         // Max reasoning loops (default: 10)
  maxTokens?: number;            // Token limit per request
  temperature?: number;          // LLM temperature (default: 0.7)
  logLevel?: LogLevel;           // Logging verbosity
  context?: ContextOptions;      // Multimodal context settings
}
```

### Core Methods

#### `agent.run(input)`

Execute the agent with text input:

```typescript
const response = await agent.run('Your question here');
```

#### `agent.run(options)`

Execute with advanced options:

```typescript
interface AgentRunOptions {
  input: string | ChatCompletionMessageParam[];
  stream?: boolean;              // Enable streaming
  sessionId?: string;           // Session identifier
  model?: string;               // Override model
  provider?: string;            // Override provider
  abortSignal?: AbortSignal;    // Cancellation support
}
```

#### `agent.registerTool(tool)`

Add tools dynamically:

```typescript
agent.registerTool({
  name: 'calculate',
  description: 'Perform mathematical calculations',
  parameters: { /* JSON schema */ },
  execute: async (params) => { /* implementation */ }
});
```

#### `agent.getLLMClient()`

Access the underlying LLM client for direct API calls:

```typescript
const client = agent.getLLMClient();
const response = await agent.callLLM({
  messages: [{ role: 'user', content: 'Hello' }]
});
```

#### `agent.getEventStream()`

Access real-time execution events:

```typescript
const stream = agent.getEventStream();
stream.on('assistant_message', (event) => {
  console.log('Agent response:', event.content);
});
```

### Event Types

- `agent_run_start` - Execution begins
- `user_message` - User input received
- `assistant_message` - Agent response
- `tool_call` - Tool execution
- `agent_run_end` - Execution complete

## Publishing Agents

<!-- [PLACEHOLDER: Add publishing workflow documentation] -->

### Package Structure

When publishing your agent as an npm package:

```
my-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts          # Main agent export
â”‚   â”œâ”€â”€ tools/            # Custom tools
â”‚   â””â”€â”€ prompts/          # Prompt templates
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

### Example Package

```typescript
// src/index.ts
import { Agent, AgentOptions } from '@tarko/agent';
import { myCustomTool } from './tools';

export class MySpecializedAgent extends Agent {
  constructor(options: AgentOptions = {}) {
    super({
      ...options,
      instructions: 'I am a specialized agent for...',
      tools: [myCustomTool, ...(options.tools || [])]
    });
  }
}

export default MySpecializedAgent;
```

## Deployment

<!-- [PLACEHOLDER: Add deployment guide with Docker, serverless examples] -->

### Docker Deployment

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
EXPOSE 3000
CMD ["npm", "start"]
```

### Environment Variables

```bash
# Required for OpenAI
OPENAI_API_KEY=your_api_key

# For Anthropic
ANTHROPIC_API_KEY=your_api_key

# Optional
LOG_LEVEL=info
MAX_ITERATIONS=10
```

## Running Agents

### Development Mode

```bash
# Install dependencies
npm install

# Run with hot reload
npm run dev

# Run tests
npm test
```

### Production

```bash
# Build the project
npm run build

# Start production server
npm start
```

### Integration Examples

#### Express.js Server

```typescript
import express from 'express';
import { Agent } from '@tarko/agent';

const app = express();
const agent = new Agent({ /* config */ });

app.post('/chat', async (req, res) => {
  try {
    const response = await agent.run(req.body.message);
    res.json({ response: response.content });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
```

#### WebSocket Streaming

```typescript
import WebSocket from 'ws';

const wss = new WebSocket.Server({ port: 8080 });

wss.on('connection', (ws) => {
  ws.on('message', async (data) => {
    const { message } = JSON.parse(data.toString());
    
    for await (const event of await agent.run({ 
      input: message, 
      stream: true 
    })) {
      ws.send(JSON.stringify(event));
    }
  });
});
```

## Advanced Configuration

### Custom Model Providers

```typescript
const agent = new Agent({
  model: {
    providers: [
      {
        name: 'custom-provider',
        baseURL: 'https://api.custom-llm.com/v1',
        api_key: process.env.CUSTOM_API_KEY
      }
    ],
    provider: 'custom-provider',
    id: 'custom-model-v1'
  }
});
```

### Context Awareness

```typescript
const agent = new Agent({
  context: {
    maxImagesCount: 10,        // Max images in context
    retainHistory: true,       // Keep conversation history
    summarizeAfter: 50         // Summarize after N messages
  }
});
```

### Tool Call Engines

```typescript
const agent = new Agent({
  toolCallEngine: 'native',    // 'native' | 'prompt-engineering' | 'structured-outputs'
  enableStreamingToolCallEvents: true
});
```

## Tarko Ecosystem

**Tarko** is a comprehensive framework for building AI applications. `@tarko/agent` integrates seamlessly with other Tarko components:

- **[@tarko/model-provider](https://www.npmjs.com/package/@tarko/model-provider)** - Multi-provider LLM abstraction
- **[@tarko/shared-utils](https://www.npmjs.com/package/@tarko/shared-utils)** - Common utilities and logging
- **[@tarko/agent-interface](https://www.npmjs.com/package/@tarko/agent-interface)** - Type definitions and contracts
- **[@tarko/llm-client](https://www.npmjs.com/package/@tarko/llm-client)** - Low-level LLM communication

<!-- [PLACEHOLDER: Add links to other Tarko documentation] -->



## Contributing

Contributions are welcome! Please read our [contributing guidelines](../../CONTRIBUTING.md) before submitting PRs.

## License

Apache-2.0 - see [LICENSE](https://github.com/bytedance/UI-TARS-desktop/blob/main/LICENSE) for details.
